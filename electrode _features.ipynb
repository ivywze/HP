{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import os.path\n",
    "import mne\n",
    "import resampy\n",
    "# !pip install librosa\n",
    "import librosa\n",
    "from scipy.signal import resample, hann\n",
    "#!pip install PyWavelets\n",
    "import pywt\n",
    "from sklearn import preprocessing\n",
    "from scipy import signal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = ['clips/www.isip.piconepress.com/normal/tuh_eeg/downloads/tuh_eeg_seizure/v1.5.2/edf/train/01_tcp_ar/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_key(file_name):\n",
    "    \"\"\" sort the file name by session \"\"\"\n",
    "    return re.findall(r'(s\\d{2})', file_name)\n",
    "\n",
    "\n",
    "def natural_key(file_name):\n",
    "    \"\"\" provides a human-like sorting key of a string \"\"\"\n",
    "    key = [int(token) if token.isdigit() else None\n",
    "           for token in re.split(r'(\\d+)', file_name)]\n",
    "    return key\n",
    "\n",
    "def time_key(file_name):\n",
    "    \"\"\" provides a time-based sorting key \"\"\"\n",
    "    splits = file_name.split('/')\n",
    "    [date] = re.findall(r'(\\d{4}_\\d{2}_\\d{2})', splits[-2])\n",
    "    date_id = [int(token) for token in date.split('_')]\n",
    "    recording_id = natural_key(splits[-1])\n",
    "    session_id = session_key(splits[-2])\n",
    "\n",
    "    return date_id + session_id + recording_id\n",
    "\n",
    "\n",
    "def read_all_file_names(path, extension, key=\"time\"):\n",
    "    \"\"\" read all files with specified extension from given path\n",
    "    :param path: parent directory holding the files directly or in subdirectories\n",
    "    :param extension: the type of the file, e.g. '.txt' or '.edf'\n",
    "    :param key: the sorting of the files. natural e.g. 1, 2, 12, 21 (machine 1, 12, 2, 21) or by time since this is\n",
    "    important for cv. time is specified in the edf file names\n",
    "    \"\"\"\n",
    "    file_paths = glob.glob(path + '**/*' + extension, recursive=True)\n",
    "\n",
    "    if key == 'time':\n",
    "        return sorted(file_paths, key=time_key)\n",
    "\n",
    "    elif key == 'natural':\n",
    "        return sorted(file_paths, key=natural_key)\n",
    "    \n",
    "def get_all_sorted_file_names_and_labels_by_channel(full_folder):\n",
    "    all_file_names = []\n",
    "    for folder in full_folder:\n",
    "        full_folder = os.path.join(folder) + '/'\n",
    "        #log.info(\"Reading {:s}...\".format(full_folder))\n",
    "        this_file_names = read_all_file_names(full_folder, '.edf', key='time')\n",
    "        #log.info(\".. {:d} files.\".format(len(this_file_names)))\n",
    "        all_file_names.extend(this_file_names)\n",
    "    #log.info(\"{:d} files in total.\".format(len(all_file_names)))\n",
    "    all_file_names = sorted(all_file_names, key=time_key)\n",
    "\n",
    "    labels = ['/abnormal/' in f for f in all_file_names]\n",
    "    labels = np.array(labels).astype(np.int64)\n",
    "    return all_file_names, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names, labels = get_all_sorted_file_names_and_labels_by_channel(data_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given full link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = []\n",
    "\n",
    "with open('file.txt', 'r') as src:  \n",
    "     for line in src:\n",
    "            line = line.replace('https://', '/Users/').rstrip('\\n')\n",
    "            if line not in all_file:\n",
    "                all_file.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = all_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sorted_file_names_and_labels_by_type(full_folder):\n",
    "    all_file_names = []\n",
    "    for folder in full_folder:\n",
    "        this_file_names = os.path.join(folder)\n",
    "        all_file_names.append(this_file_names)\n",
    "    all_file_names = sorted(all_file_names)\n",
    "\n",
    "    labels = ['/tuh_eeg/' in f for f in all_file_names] \n",
    "    labels = np.array(labels).astype(np.int64)\n",
    "    return all_file_names, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names, labels = get_all_sorted_file_names_and_labels_by_type(data_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_recording_mins = 35  # exclude larger recordings from training set\n",
    "n_recordings = None  # set to an integer, if you want to restrict the set size\n",
    "sensor_types = [\"EEG\"]\n",
    "sec_to_cut = 60  # cut away at start of each recording\n",
    "duration_recording_mins = 20  # how many minutes to use per recording\n",
    "max_abs_val = 800  # for clipping\n",
    "sampling_freq = 256 # resample\n",
    "divisor = 10  # divide signal by this\n",
    "max_hz = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recording_length(file_path):\n",
    "    \"\"\" some recordings were that huge that simply opening them with mne caused the program to crash. therefore, open\n",
    "    the edf as bytes and only read the header. parse the duration from there and check if the file can safely be opened\n",
    "    :param file_path: path of the directory\n",
    "    :return: the duration of the recording\n",
    "    \"\"\"\n",
    "    f = open(file_path, 'rb')\n",
    "    header = f.read(256)\n",
    "    f.close()\n",
    "\n",
    "    return int(header[236:244].decode('ascii'))\n",
    "\n",
    "\n",
    "\n",
    "def get_recording_detail(file_path):\n",
    "    \"\"\" # renew \n",
    "    :param file_path: path of the directory\n",
    "    :return: the duration of the recording, birth year of patient, gender of patient\n",
    "    \"\"\"\n",
    "    f = open(file_path, 'rb')\n",
    "    header = f.read(256)\n",
    "    length = int(header[236:244].decode('ascii'))\n",
    "    year = int(header[26:30].decode('ascii'))\n",
    "    gender = str(header[17:18].decode('ascii'))\n",
    "    f.close()\n",
    "\n",
    "    return length, year, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_recording_mins is not None:\n",
    "    lengths = [get_recording_length(fname) for fname in all_file_names]\n",
    "    lengths = np.array(lengths)\n",
    "    mask = lengths < max_recording_mins * 60\n",
    "    cleaned_file_names = np.array(all_file_names)[mask]\n",
    "    cleaned_labels = labels[mask]\n",
    "else:\n",
    "    cleaned_file_names = np.array(all_file_names)\n",
    "    cleaned_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, int(sec_to_cut * fs):-int(sec_to_cut * fs)], fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_with_mne(file_path):\n",
    "    \"\"\" read info from the edf file without loading the data. loading data is done in multiprocessing since it takes\n",
    "    some time. getting info is done before because some files had corrupted headers or weird sampling frequencies\n",
    "    that caused the multiprocessing workers to crash. therefore get and check e.g. sampling frequency and duration\n",
    "    beforehand\n",
    "    :param file_path: path of the recording file\n",
    "    :return: file name, sampling frequency, number of samples, number of signals, signal names, duration of the rec\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edf_file = mne.io.read_raw_edf(file_path, verbose='error')\n",
    "    except ValueError:\n",
    "        return None, None, None, None, None, None\n",
    "    sampling_frequency = int(edf_file.info['sfreq'])\n",
    "    if sampling_frequency < 10:\n",
    "        sampling_frequency = 1 / (edf_file.times[1] - edf_file.times[0])\n",
    "        if sampling_frequency < 10:\n",
    "            return None, sampling_frequency, None, None, None, None\n",
    "\n",
    "    n_samples = edf_file.n_times\n",
    "    signal_names = edf_file.ch_names\n",
    "    n_signals = len(signal_names)\n",
    "    duration = n_samples / max(sampling_frequency, 1)\n",
    "\n",
    "    return edf_file, sampling_frequency, n_samples, n_signals, signal_names, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname, preproc_functions):\n",
    "    cnt, sfreq, n_samples, n_channels, chan_names, n_sec = get_info_with_mne(\n",
    "        fname)\n",
    "    log.info(\"Load data...\")\n",
    "    cnt.load_data()\n",
    "    selected_ch_names = []\n",
    "    wanted_elecs = ['C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2',\n",
    "                    'P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6'] #'A1', 'A2' --> not in 03\n",
    " \n",
    "    for wanted_part in wanted_elecs:\n",
    "        wanted_found_name = []\n",
    "        for ch_name in cnt.ch_names:\n",
    "            if ' ' + wanted_part + '-' in ch_name:\n",
    "                wanted_found_name.append(ch_name)\n",
    "        assert len(wanted_found_name) == 1\n",
    "        selected_ch_names.append(wanted_found_name[0])\n",
    "\n",
    "    cnt = cnt.pick_channels(selected_ch_names)\n",
    "\n",
    "    assert np.array_equal(sorted(cnt.ch_names), sorted(selected_ch_names))\n",
    "    \n",
    "    n_sensors = 0\n",
    "    n_sensors += 19\n",
    "\n",
    "    assert len(cnt.ch_names)  == n_sensors, (\n",
    "        \"Expected {:d} channel names, got {:d} channel names\".format(\n",
    "            n_sensors, len(cnt.ch_names)))\n",
    "\n",
    "    # change from volt to mikrovolt\n",
    "    data = (cnt.get_data() * 1e6).astype(np.float32)\n",
    "    fs = cnt.info['sfreq']\n",
    "    log.info(\"Preprocessing...\")\n",
    "    for fn in preproc_functions:\n",
    "        log.info(fn)\n",
    "        data, fs = fn(data, fs)\n",
    "        data = data.astype(np.float32)\n",
    "        fs = float(fs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "n_files = len(cleaned_file_names[:n_recordings])\n",
    "for i_fname, fname in enumerate(cleaned_file_names[:n_recordings]):\n",
    "    log.info(\"Load {:d} of {:d}\".format(i_fname + 1,n_files))\n",
    "    x = load_data(fname, preproc_functions=preproc_functions)\n",
    "    assert x is not None\n",
    "    X.append(x)\n",
    "    y.append(cleaned_labels[i_fname])\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X[0]\n",
    "for i in range(1, len(X)):\n",
    "    data = np.append(data, X[i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT(data):\n",
    "    \"\"\"\n",
    "    Apply Fast Fourier Transform to the last axis.\n",
    "    \"\"\"\n",
    "    axis = data.ndim - 1\n",
    "    return np.fft.rfft(data, axis=axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log10(data):\n",
    "    \"\"\"\n",
    "    Apply Log10\n",
    "    \"\"\"\n",
    "    # 10.0 * log10(re * re + im * im)\n",
    "    indices = np.where(data <= 0)\n",
    "    data[indices] = np.max(data)\n",
    "    data[indices] = (np.min(data) * 0.1)\n",
    "    return np.log10(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Magnitude(data):\n",
    "    return np.absolute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MagnitudeAndPhase(data):\n",
    "    \"\"\"\n",
    "    Take the magnitudes and phases of complex data and append them together.\n",
    "    \"\"\"\n",
    "    magnitudes = np.absolute(data)\n",
    "    phases = np.angle(data)\n",
    "    return np.concatenate((magnitudes, phases), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_channel(data):\n",
    "    \"\"\"\n",
    "    Take (mean,standard_deviation, min, max) for each channel.\n",
    "    \"\"\"\n",
    "    # data[ch][dim]\n",
    "    shape = data.shape\n",
    "    out = np.empty((shape[0], 4))\n",
    "    for i in range(len(data)):\n",
    "        ch_data = data[i]\n",
    "        outi = out[i]\n",
    "        outi[0] = np.mean(ch_data)\n",
    "        outi[1] = np.std(ch_data)\n",
    "        outi[2] = np.min(ch_data)\n",
    "        outi[3] = np.max(ch_data)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(data):\n",
    "    \"\"\"\n",
    "    Take (mean,standard_deviation, min, max) for data.\n",
    "    \"\"\"\n",
    "    # data[ch][dim]\n",
    "    \n",
    "    out = []\n",
    "    out.append(np.mean(data))\n",
    "    out.append(np.std(data))\n",
    "    out.append(np.min(data))\n",
    "    out.append(np.max(data))\n",
    "\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(data):\n",
    "    \"\"\"\n",
    "    Mel-frequency cepstrum coefficients\n",
    "    \"\"\"\n",
    "    all_mfccs = []\n",
    "    for ch in data:\n",
    "        mfccs = librosa.feature.mfcc(y=ch,n_mfcc = 13, sr=sampling_freq)[2:13] \n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs,order=2)\n",
    "        comprehensice_mfccs = np.concatenate((mfccs, delta_mfccs, delta2_mfccs),axis = 0)\n",
    "        all_mfccs.append(comprehensice_mfccs.ravel())\n",
    "\n",
    "    return np.array(all_mfccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resample(data):\n",
    "    \"\"\"\n",
    "    Resample time-series data.\n",
    "    \"\"\"\n",
    "    axis = data.ndim - 1\n",
    "    if data.shape[-1] > sampling_freq:\n",
    "        return resample(data, sampling_freq, axis=axis)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResampleHanning(data):\n",
    "    \"\"\"\n",
    "    Resample time-series data using a Hanning window\n",
    "    \"\"\"\n",
    "    axis = data.ndim - 1\n",
    "    out = resample(data, sampling_freq, axis=axis, window=hann(M=data.shape[axis]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnitScale(data):\n",
    "    \"\"\"\n",
    "    Scale across the last axis.\n",
    "    \"\"\"\n",
    "    return preprocessing.scale(data, axis=data.ndim-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnitScaleFeat(data):\n",
    "    \"\"\"\n",
    "    Scale across the first axis, i.e. scale each feature.\n",
    "    \"\"\"\n",
    "    return preprocessing.scale(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationMatrix(data):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients matrix across all EEG channels.\n",
    "    \"\"\"\n",
    "    return np.corrcoef(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eigenvalues(data):\n",
    "    \"\"\"\n",
    "    Take eigenvalues of a matrix, and sort them by magnitude in order to\n",
    "    make them useful as features (as they have no inherent order).\n",
    "    \"\"\"\n",
    "    w, v = np.linalg.eig(data)\n",
    "    w = np.absolute(w)\n",
    "    w.sort()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the upper right triangle of a matrix\n",
    "def upper_right_triangle(matrix):\n",
    "    accum = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(i+1, matrix.shape[1]):\n",
    "            accum.append(matrix[i, j])\n",
    "\n",
    "    return np.array(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Freq_Correlation(data):\n",
    "    \n",
    "        \"\"\"\n",
    "        Correlation in the frequency domain. First take FFT with (start, end) slice options,\n",
    "        then calculate correlation co-efficients on the FFT output, followed by calculating\n",
    "        eigenvalues on the correlation co-efficients matrix.\n",
    "        The output features are ( upper_right_diagonal(correlation_coefficients)\n",
    "        Features can be selected/omitted using the constructor arguments.\n",
    "        \"\"\"\n",
    "        data1 = FFT(data)\n",
    "        data1 = Magnitude(data1)\n",
    "        data1 = Log10(data1)\n",
    "\n",
    "        data2 = data1\n",
    "\n",
    "        data2 = UnitScaleFeat(data2)\n",
    "        \n",
    "        data2 = CorrelationMatrix(data2)\n",
    "\n",
    "        w = Eigenvalues(data2)\n",
    "\n",
    "        out = []\n",
    "        data2 = upper_right_triangle(data2)\n",
    "        out.append(data2)\n",
    "\n",
    "        for d in out:\n",
    "            assert d.ndim == 1\n",
    "\n",
    "        return np.concatenate(out, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time_Correlation(data):\n",
    "    \"\"\"\n",
    "    Correlation in the time domain. First downsample the data, then calculate correlation co-efficients\n",
    "    followed by calculating eigenvalues on the correlation co-efficients matrix.\n",
    "    The output features are (upper_right_diagonal(correlation_coefficients), eigenvalues)\n",
    "    Features can be selected/omitted using the constructor arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    # so that correlation matrix calculation doesn't crash\n",
    "    for ch in data:\n",
    "        if np.alltrue(ch == 0.0):\n",
    "            ch[-1] += 0.00001\n",
    "\n",
    "    data1 = data\n",
    "    if data1.shape[1] > max_hz: \n",
    "        data1 = Resample(data1)\n",
    "\n",
    "    data1 = UnitScaleFeat(data1)\n",
    "\n",
    "    data1 = CorrelationMatrix(data1)\n",
    "\n",
    "    out = []\n",
    "    data1 = upper_right_triangle(data1)\n",
    "    out.append(data1)\n",
    "\n",
    "    for d in out:\n",
    "        assert d.ndim == 1\n",
    "\n",
    "    return np.concatenate(out, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeFreqCorrelation(data):\n",
    "    \"\"\"\n",
    "    Combines time and frequency correlation, taking both correlation coefficients and eigenvalues.\n",
    "    \"\"\"\n",
    "    data1 = TimeCorrelation(data)\n",
    "    data2 = FreqCorrelation(data)\n",
    "    assert data1.ndim == data2.ndim\n",
    "    return np.concatenate((data1, data2), axis=data1.ndim-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f=open('./descriptive/stats_TCSZ.txt','wb')\n",
    "f.write(b\"stats of data \\n\")\n",
    "np.savetxt(f,stats_all,newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"stats of mfccs \\n\")\n",
    "np.savetxt(f,stats_mfccs, newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"stats of Freq Correlation \\n\")\n",
    "np.savetxt(f, stats(FreqCorrelation), newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"stats of Time Correlation \\n\")\n",
    "np.savetxt(f, stats(TimeCorrelation), newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"Eigenvalues \\n\")\n",
    "np.savetxt(f, eigenvalues, newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"stats of channel \\n\")\n",
    "np.savetxt(f,stats_channels, newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"FreqCorrelation \\n\")\n",
    "np.savetxt(f,FreqCorrelation, newline=\", \")\n",
    "f.write(b\"\\n\\n\")\n",
    "f.write(b\"Time Correlation \\n\")\n",
    "np.savetxt(f,TimeCorrelation, newline=\", \")\n",
    "\n",
    "f.close()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
