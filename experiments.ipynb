{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "import mne\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np     \n",
    "import nltk, string\n",
    "from scipy.signal import resample, hann\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(document):\n",
    "    punctuation=\"[!@#$%^&*()_+{}:\\\"<>?,./;“”‘’\\n]+\"\n",
    "    document = re.sub(punctuation, ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    else:\n",
    "        raise ValueError('Document is not string!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_wave(info_file):\n",
    "    info_content = open(info_file,'r').read()\n",
    "    sentences = parse_document(info_content) \n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences] # tokenize sentences\n",
    "    tokens = [token.lower() for token in tokenized_sentences[0]] # lower tokens\n",
    "    \n",
    "    waves_list = ['gamma', 'beta', 'alpha', 'theta','delta']\n",
    "    info_wave = []\n",
    "\n",
    "    for i in waves_list:\n",
    "        if any(i in waves for waves in tokens):\n",
    "            info_wave.append(i)\n",
    "            \n",
    "    return info_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sampling_frequency(edf_filename):\n",
    "    edf = mne.io.read_raw_edf(edf_filename,stim_channel=None)\n",
    "    sampling_frequency = int(edf.info['sfreq'])\n",
    "    \n",
    "    return sampling_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_and_labels(edf_filename, summary_text):\n",
    "    folder, basename = os.path.split(edf_filename)\n",
    "    \n",
    "    edf = mne.io.read_raw_edf(edf_filename,stim_channel=None)\n",
    "    sampling_frequency = int(edf.info['sfreq'])\n",
    "    \n",
    "    edf.load_data()\n",
    "    selected_ch_names = []\n",
    "    wanted_elecs = ['C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2',\n",
    "                    'P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6','A1','A2'] \n",
    " \n",
    "    for wanted_part in wanted_elecs:\n",
    "        wanted_found_name = []\n",
    "        for ch_name in edf.ch_names:\n",
    "            if ' ' + wanted_part + '-' in ch_name:\n",
    "                wanted_found_name.append(ch_name)\n",
    "        assert len(wanted_found_name) == 1\n",
    "        selected_ch_names.append(wanted_found_name[0])\n",
    "\n",
    "    edf = edf.pick_channels(selected_ch_names)\n",
    "    \n",
    "    assert np.array_equal(sorted(edf.ch_names), sorted(selected_ch_names))\n",
    "    \n",
    "    n_sensors = 0\n",
    "    n_sensors += 21\n",
    "\n",
    "    assert len(edf.ch_names)  == n_sensors    \n",
    "    \n",
    "    X = edf.get_data().astype(np.float32) * 1e6 # to mV\n",
    "    \n",
    "    y = np.zeros(int(X.shape[1]/sampling_frequency), dtype=np.int64)\n",
    "    \n",
    "    i_text_start = summary_text.index(basename)\n",
    "\n",
    "    if 'File Name' in summary_text[i_text_start:]:\n",
    "        i_text_stop = summary_text.index('File Name', i_text_start)\n",
    "    else:\n",
    "        i_text_stop = len(summary_text)\n",
    "    assert i_text_stop > i_text_start\n",
    "\n",
    "    file_text = summary_text[i_text_start:i_text_stop]\n",
    "    \n",
    "    if 'Seizure Start' in file_text:\n",
    "        start_sec = re.findall(r\"Seizure Start Time: ([0-9]*) seconds\", summary_text)\n",
    "        end_sec = re.findall(r\"Seizure End Time: ([0-9]*) seconds\", summary_text)\n",
    "        \n",
    "        first_seizure = int((start_sec[0]))\n",
    "        last_seizure = int((end_sec[len(end_sec)-1]))\n",
    "        \n",
    "        assert len(start_sec) == len(end_sec)\n",
    "        \n",
    "        for i in range(0,len(start_sec)):\n",
    "            i_seizure_start = int((start_sec[i]))\n",
    "            i_seizure_stop = int((end_sec[i]))\n",
    "            y[i_seizure_start:i_seizure_stop] = 1\n",
    "            \n",
    "\n",
    "    X_final = []\n",
    "    y_final = []\n",
    "    start_cut = max(0,first_seizure-60)\n",
    "    last_cut = min(last_seizure+60, int(X.shape[1]/sampling_frequency))\n",
    "\n",
    "    for i in range(start_cut,last_cut): #take 60 seconds before and after seizures\n",
    "        X_final.append(X[:,sampling_frequency*i:sampling_frequency*(i+1)])\n",
    "        y_final.append(y[i])\n",
    "        \n",
    "\n",
    "    assert len(X_final) == len(y_final)\n",
    "    return X_final,y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(data):\n",
    "    axis = data.ndim - 1\n",
    "    data1 = np.fft.fft(data, axis=axis) \n",
    "    data2 = np.absolute(data1)  # take absolute value\n",
    "    data3 = data2.ravel() \n",
    "    out = []\n",
    "    out.append(data3)\n",
    "\n",
    "    for d in out:\n",
    "        assert d.ndim == 1\n",
    "            \n",
    "    return np.concatenate(out, axis=0)\n",
    "\n",
    "def get_file(edf_file_names,summary_file):\n",
    "    X_final,y = extract_data_and_labels(edf_file_names, summary_content)\n",
    "    \n",
    "    output = []\n",
    "    for i in range(0,len(X_final)):\n",
    "        final = fft(X_final[i])\n",
    "        output.append(final)\n",
    "        \n",
    "    return output, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft(snippet, Fs):\n",
    "    snippet_time = len(snippet)/Fs\n",
    "    Ts = 1.0/Fs; \n",
    "    t = np.arange(0,snippet_time,Ts) \n",
    "    y = snippet\n",
    "    n = len(y)\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T \n",
    "    frq = frq[range(n//2)] \n",
    "    Y = np.fft.fft(y) \n",
    "    Y = Y[range(n//2)]\n",
    "    return frq,abs(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_band(f,Y):\n",
    "    theta_range = (4,8)\n",
    "    alpha_range = (8,14)\n",
    "    beta_range = (14,31)\n",
    "    gamma_range = (30,50)\n",
    "    delta_range = (0,4)\n",
    "    \n",
    "    theta = Y[(f>theta_range[0]) & (f<=theta_range[1])]\n",
    "    alpha = Y[(f>alpha_range[0]) & (f<=alpha_range[1])]\n",
    "    beta = Y[(f>beta_range[0]) & (f<=beta_range[1])]\n",
    "    gamma = Y[(f>gamma_range[0]) & (f<=gamma_range[1])]\n",
    "    delta = Y[(f>delta_range[0]) & (f<=delta_range[1])]\n",
    "\n",
    "    return gamma, alpha, theta, delta, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_selected_band(selected_bands,info_wave):\n",
    "    final_selected_bands = []\n",
    "\n",
    "    for i,j in selected_bands.items():\n",
    "        if i in info_wave:\n",
    "            final_selected_bands.append(j)\n",
    "            \n",
    "    out = []\n",
    "    for i in range(0,len(final_selected_bands)):\n",
    "        out.append(final_selected_bands[i].ravel())\n",
    "\n",
    "    final = []\n",
    "    for i in range(0,len(out)):\n",
    "        for j in out[i]:\n",
    "            final.append(j)\n",
    "            \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_file(edf_file_names,summary_file,info_file):\n",
    "    summary_content = open(summary_file,'r').read()\n",
    "    X_final,y = extract_data_and_labels(edf_file_names, summary_content)\n",
    "    sampling_frequency = extract_sampling_frequency(edf_file_names)\n",
    "    \n",
    "    info_wave = get_info_wave(info_file)\n",
    "    \n",
    "    output = []\n",
    "    for i in range(0,len(X_final)):\n",
    "        f, Y = get_fft(X_final[i],sampling_frequency)\n",
    "        gamma, alpha, theta, delta, beta = selected_band(f, Y)\n",
    "        selected_bands = {'gamma': gamma, 'alpha': alpha, 'theta': theta, 'delta': delta, 'beta': beta}\n",
    "        final = final_selected_band(selected_bands,info_wave)\n",
    "        output.append(final)\n",
    "        \n",
    "        \n",
    "    return output, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_whole_file(edf_file_names,summary_file,info_file):\n",
    "    summary_content = open(summary_file,'r').read()\n",
    "    X_final,y = extract_data_and_labels(edf_file_names, summary_content)\n",
    "    sampling_frequency = extract_sampling_frequency(edf_file_names)\n",
    "    \n",
    "    info_wave = ['gamma','alpha','theta','delta','beta']\n",
    "    \n",
    "    output = []\n",
    "    for i in range(0,len(X_final)):\n",
    "        f, Y = get_fft(X_final[i],sampling_frequency)\n",
    "        gamma, alpha, theta, delta, beta = selected_band(f, Y)\n",
    "        selected_bands = {'gamma': gamma, 'alpha': alpha, 'theta': theta, 'delta': delta, 'beta': beta}\n",
    "        final = final_selected_band(selected_bands,info_wave)\n",
    "        output.append(final)\n",
    "        \n",
    "        \n",
    "    return output, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resample(data):\n",
    "    \"\"\"\n",
    "    Resample time-series data.\n",
    "    \"\"\"\n",
    "    sampling_freq = 250\n",
    "    \n",
    "    if len(data[0]) >= sampling_freq:\n",
    "        out = resample(data, sampling_freq, axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_TNSZ_file = ['00008889_s003_t010','00009044_s001_t000','00008444_s003_t001','00008444_s003_t002','00008444_s003_t003','00008444_s003_t004','00008444_s003_t005','00008444_s003_t010','00008444_s003_t012','00008444_s004_t004','00008889_s004_t000','00008889_s004_t001','00008889_s004_t002','00008889_s004_t003','00008889_s004_t004','00008889_s004_t005','00008889_s004_t006','00008889_s004_t007','00008889_s004_t008','00008889_s004_t009','00008889_s004_t010','00008889_s004_t011','00008889_s004_t012','00008889_s004_t013','00008889_s004_t014','00008889_s004_t015','00008889_s005_t004','00008889_s005_t005']\n",
    "all_TCSZ_file = ['00010158_s001_t001','00010088_s010_t001','00000906_s005_t000','00000258_s003_t002','00000258_s003_t003','00000258_s003_t004','00008889_s002_t002','00008889_s002_t003','00008889_s002_t005','00008889_s002_t006','00008889_s002_t008','00008889_s002_t010','00008889_s003_t001','00008889_s003_t006','00009578_s003_t003','00009578_s004_t001','00009578_s004_t002','00009578_s004_t004']\n",
    "all_MYSZ_file = ['00008606_s001_t000']\n",
    "all_SPSZ_file = ['00006546_s021_t002','00006546_s024_t000','00006546_s024_t001','00008527_s003_t003','00008527_s004_t000','00008527_s004_t002','00008527_s004_t003','00008616_s001_t000']\n",
    "all_ABSZ_file = ['00008608_s001_t000']\n",
    "all_CPSZ_file = ['00000883_s002_t000','00006904_s004_t002','00008544_s002_t004','00006904_s004_t003','00008544_s004_t006','00006904_s004_t004','00008544_s004_t007','00006904_s005_t000','00008544_s004_t008','00001981_s009_t001','00006904_s005_t001','00008544_s004_t010','00002806_s001_t001','00006904_s005_t002','00008544_s004_t011','00002806_s001_t002','00006904_s007_t000','00008544_s004_t012','00002806_s001_t003','00006904_s007_t001','00008544_s005_t000','00002806_s001_t004','00006904_s007_t002','00008544_s005_t001','00002806_s001_t006','00006904_s007_t003','00008544_s005_t002','00002806_s001_t007','00006904_s007_t004','00008544_s005_t003','00004456_s012_t002','00006904_s007_t005','00008544_s005_t005',\n",
    "'00004456_s012_t003','00006904_s007_t006','00008544_s005_t006','00005479_s003_t000','00006904_s008_t001','00008544_s005_t007','00005479_s004_t000','00006904_s008_t002','00006535_s005_t007','00006986_s001_t001','00006535_s006_t006','00008295_s001_t000','00006546_s025_t003','00008345_s001_t000','00008615_s001_t000','00006811_s001_t000','00008453_s005_t000','00013407_s001_t000','00008460_s001_t000','00013407_s001_t004',\n",
    "'00006904_s004_t000','00008544_s001_t000','00013407_s001_t013','00006904_s004_t001','00008544_s002_t001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all = []\n",
    "y_all = []\n",
    "\n",
    "\n",
    "for i in all_XXXX_file[:]:\n",
    "    edf_file_names = \"../clips/{}/{}.edf\".format(i,i)\n",
    "    summary_file = \"../clips/{}/{}_summary.txt\".format(i,i)\n",
    "    info_file = \"../clips/{}/{}.txt\".format(i,i)\n",
    "    \n",
    "    summary_content = open(summary_file,'r').read()\n",
    "    X_final,y = extract_data_and_labels(edf_file_names, summary_content)\n",
    "    \n",
    "    #input 1\n",
    "    output, y = get_file(edf_file_names,summary_file)\n",
    "    #input 2\n",
    "    output, y = get_one_whole_file(edf_file_names,summary_file,info_file)\n",
    "    #input 3\n",
    "    output, y = get_one_file(edf_file_names,summary_file,info_file)\n",
    "    \n",
    "    if len(output[0]) != 0:\n",
    "        output = Resample(output)\n",
    "        #combine\n",
    "        output_all.append(output.tolist())\n",
    "        y_all.append(y)\n",
    "\n",
    "\n",
    "output_final = []\n",
    "y_final = []\n",
    "\n",
    "for i in range(0, len(y_all)):\n",
    "    for j in range(0,len(y_all[i])):\n",
    "        y_final.append(y_all[i][j])\n",
    "        \n",
    "y_final = np.array(y_final)\n",
    "\n",
    "for i in range(0, len(output_all)):\n",
    "    for j in range(0,len(output_all[i])):\n",
    "        output_final.append(output_all[i][j])\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "        'ACC': 'accuracy',\n",
    "        'AUC': 'roc_auc',\n",
    "        'sensitivity': make_scorer(recall_score),\n",
    "        'specificity': make_scorer(recall_score,pos_label=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVC = SVC(C= 1000,kernel='rbf',gamma= 1e-09,random_state = 0 )\n",
    "\n",
    "scores_SVC = []\n",
    "score_SVC = cross_validate(clf_SVC, output_final, y_final, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring= scoring)#.mean()\n",
    "scores_SVC.append(score_SVC['test_ACC'].mean())\n",
    "scores_SVC.append(score_SVC['test_AUC'].mean())\n",
    "scores_SVC.append(score_SVC['test_sensitivity'].mean())\n",
    "scores_SVC.append(score_SVC['test_specificity'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, bootstrap=False, random_state=0)\n",
    "\n",
    "scores_RF = []\n",
    "\n",
    "score_RF = cross_validate(clf_RF, output_final, y_final, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring= scoring)\n",
    "scores_RF.append(score_RF['test_ACC'].mean())\n",
    "scores_RF.append(score_RF['test_AUC'].mean())\n",
    "scores_RF.append(score_RF['test_sensitivity'].mean())\n",
    "scores_RF.append(score_RF['test_specificity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
